{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da739e39",
   "metadata": {},
   "source": [
    "# Hybrid GPT/SSM fine-tuning\n",
    "Based on: https://unsloth.ai/docs/models/tutorials/ibm-granite-4.0#fine-tuning-granite-4.0-in-unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, importlib.util\n",
    "import platform\n",
    "\n",
    "if platform.system() != \"Linux\":\n",
    "    print(\"mamba_ssm currently only runs on linux\")\n",
    "    raise\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    print(\"Torch missing or running on google colab, installing adjusted versions\")\n",
    "    try: import numpy, PIL; _numpy = f\"numpy=={numpy.__version__}\"; _pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: _numpy = \"numpy\"; _pil = \"pillow\"\n",
    "    !pip install -qqq \\\n",
    "        \"torch==2.7.1\" \"triton>=3.3.0\" {_numpy} {_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\"\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    print(\"Installing unsloth\")\n",
    "    !pip install -qqq unsloth\n",
    "    print(\"Installing secondary dependencies\")\n",
    "    !pip install -qqq --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth_zoo\n",
    "    print(\"Installing mamba_ssm\")\n",
    "    # Mamba is supported only on torch==2.7.1. If you have newer torch versions, please wait 30 minutes!\n",
    "    !pip install -qqq --no-build-isolation mamba_ssm==2.2.5 causal_conv1d==1.5.2 tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90545c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from utils.gpt_utils import Encoding\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/granite-4.0-micro\",\n",
    "    \"unsloth/granite-4.0-h-micro\",\n",
    "    \"unsloth/granite-4.0-h-tiny\",\n",
    "    \"unsloth/granite-4.0-h-small\",\n",
    "    # Base pretrained Granite 4 models\n",
    "    \"unsloth/granite-4.0-micro-base\",\n",
    "    \"unsloth/granite-4.0-h-micro-base\",\n",
    "    \"unsloth/granite-4.0-h-tiny-base\",\n",
    "    \"unsloth/granite-4.0-h-small-base\",\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\",  # [NEW] We support TTS models!\n",
    "]  # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/granite-4.0-h-micro\",\n",
    "    max_seq_length=2048,  # Choose any for long context!\n",
    "    load_in_4bit=True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit=False,  # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning=False,  # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"shared_mlp.input_linear\",\n",
    "        \"shared_mlp.output_linear\",\n",
    "    ],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Use the below shared sheet\n",
    "# sheet_url = 'https://docs.google.com/spreadsheets/d/1NrjI5AGNIwRtKTAse5TW_hWq2CwAS03qCHif6vaaRh0/export?format=csv&gid=0'\n",
    "\n",
    "# Or unsloth/Support-Bot-Recommendation\n",
    "sheet_url = \"https://huggingface.co/datasets/unsloth/Support-Bot-Recommendation/raw/main/support_recs.csv\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": sheet_url},\n",
    "    column_names=[\n",
    "        \"snippet\",\n",
    "        \"recommendation\",\n",
    "    ],  # Replace with the actual column names of your sheet\n",
    "    skiprows=1,  # skip header rows\n",
    ")[\"train\"]\n",
    "def formatting_prompts_func(examples):\n",
    "    user_texts = examples[\"snippet\"]\n",
    "    response_texts = examples[\"recommendation\"]\n",
    "    messages = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": response_text},\n",
    "        ]\n",
    "        for user_text, response_text in zip(user_texts, response_texts)\n",
    "    ]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            message, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        for message in messages\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "assert(isinstance(dataset, Dataset))\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "sample = dataset[5]\n",
    "print(\"Snippet\", sample[\"snippet\"])\n",
    "print(\"Recommendation\", sample[\"recommendation\"])\n",
    "print(\"Text\", sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer.sft_trainer import SFTTrainer\n",
    "from trl.trainer.sft_config import SFTConfig\n",
    "from transformers import PreTrainedTokenizerBase, ProcessorMixin\n",
    "assert(isinstance(dataset, Dataset))\n",
    "assert(isinstance(tokenizer, PreTrainedTokenizerBase) or isinstance(tokenizer, ProcessorMixin))\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,  # Can set up evaluation!\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Use GA to mimic batch size!\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,  # Reduce to 2e-5 for long training runs\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",  # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_of_role|>user<|end_of_role|>\",\n",
    "    response_part=\"<|start_of_role|>assistant<|end_of_role|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    [\n",
    "        tokenizer.pad_token_id if x == -100 else x\n",
    "        for x in trainer.train_dataset[100][\"labels\"]\n",
    "    ]\n",
    ").replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5db8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test Scenarios\n",
    "# --- Scenario 1: Video-Conferencing Screen-Share Bug (11 turns) ---\n",
    "scenario_1 = \"\"\"\n",
    "User: Everyone in my meeting just sees a black screen when I share.\n",
    "Agent: Sorry about that—are you sharing a window or your entire screen?\n",
    "User: Entire screen on macOS Sonoma.\n",
    "Agent: Thanks. Do you have “Enable hardware acceleration” toggled on in Settings → Video?\n",
    "User: Yeah, that switch is on.\n",
    "Agent: Could you try toggling it off and start a quick test share?\n",
    "User: Did that—still black for attendees.\n",
    "Agent: Understood. Are you on the desktop app v5.4.2 or the browser client?\n",
    "User: Desktop v5.4.2—just updated this morning.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 2: Smart-Lock Low-Battery Loop (9 turns) ---\n",
    "scenario_2 = \"\"\"\n",
    "User: I changed the batteries, but the lock app still says 5 % and won’t auto-lock.\n",
    "Agent: Let’s check firmware. In the app, go to Settings → Device Info—what version shows?\n",
    "User: 3.18.0-alpha.\n",
    "Agent: Latest stable is 3.17.5. Did you enroll in the beta program?\n",
    "User: I might have months ago.\n",
    "Agent: Beta builds sometimes misreport battery. Remove one battery, wait ten seconds, reinsert, and watch the LED pattern.\n",
    "User: LED blinks blue twice, then red once.\n",
    "Agent: That blink code means “config mismatch.” Do you still have the old batteries handy?\n",
    "User: Tossed them already.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 3: Accounting SaaS — Corrupted Invoice Export (10 turns) ---\n",
    "scenario_3 = \"\"\"\n",
    "User: Every invoice I download today opens as a blank PDF.\n",
    "Agent: Is this happening to historic invoices, new ones, or both?\n",
    "User: Both. Anything I export is 0 bytes.\n",
    "Agent: Are you exporting through “Bulk Actions” or individual invoice pages?\n",
    "User: Individual pages.\n",
    "Agent: Which browser/OS combo?\n",
    "User: Chrome on Windows 11, latest update.\n",
    "Agent: We released a new PDF renderer at 10 a.m. UTC. Could you try Edge quickly, just to rule out a caching quirk?\n",
    "User: Tried Edge—same zero-byte file.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 4: Fitness-Tracker App — Stuck Step Count (8 turns) ---\n",
    "scenario_4 = \"\"\"\n",
    "User: My step count has been frozen at 4,237 since last night.\n",
    "Agent: Which phone are you syncing with?\n",
    "User: iPhone 15, iOS 17.5.\n",
    "Agent: In the Health Permissions screen, does “Motion & Fitness” show as ON?\n",
    "User: Yes, it’s toggled on.\n",
    "Agent: When you pull down to refresh the dashboard, does the sync spinner appear?\n",
    "User: Spinner flashes for a second, then nothing changes.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 5: Online-Course Platform — Quiz Submission Error (12 turns) ---\n",
    "scenario_5 = \"\"\"\n",
    "User: My quiz submits but then shows “Unknown grading error” and resets the answers.\n",
    "Agent: Which course and quiz name?\n",
    "User: History 301, Unit 2 Quiz.\n",
    "Agent: Do you notice a red banner or any code like GR-### in the corner?\n",
    "User: Banner says “GR-412”.\n",
    "Agent: That code points to answer-payload size. Were you pasting images or long text into any answers?\n",
    "User: Maybe a long essay—about 800 words in Question 5.\n",
    "Agent: Are you on a laptop or mobile?\n",
    "User: Laptop, Safari on macOS.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838da48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": scenario_1},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,  # Increase if tokens are getting cut off\n",
    "    use_cache=True,\n",
    "    # Adjust the sampling params to your preference\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac61750",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": scenario_2},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,  # Increase if tokens are getting cut off\n",
    "    use_cache=True,\n",
    "    # Adjust the sampling params to your preference\n",
    "    do_sample=False,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"granite_lora\")  # Local saving\n",
    "tokenizer.save_pretrained(\"granite_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9850c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"granite_lora\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\n",
    "        \"granite_finetune_16bit\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "    )\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_USERNAME/granite_finetune_16bit\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "        token=\"YOUR_HF_TOKEN\",\n",
    "    )\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\n",
    "        \"granite_finetune_4bit\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_4bit\",\n",
    "    )\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_USERNAME/granite_finetune_4bit\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_4bit\",\n",
    "        token=\"YOUR_HF_TOKEN\",\n",
    "    )\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"granite_lora\")\n",
    "    tokenizer.save_pretrained(\"granite_lora\")\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub(\"HF_USERNAME/granite_lora\", token=\"YOUR_HF_TOKEN\")\n",
    "    tokenizer.push_to_hub(\"HF_USERNAME/granite_lora\", token=\"YOUR_HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7e1ce",
   "metadata": {},
   "source": [
    "## GGUF / llama.cpp conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a503c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\n",
    "        \"granite_finetune\",\n",
    "        tokenizer,\n",
    "    )\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"HF_USERNAME/granite_finetune\", tokenizer, token=\"YOUR_HF_TOKEN\"\n",
    "    )\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"granite_finetune\", tokenizer, quantization_method=\"f16\")\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\n",
    "        \"HF_USERNAME/granite_finetune\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"f16\",\n",
    "        token=\"YOUR_HF_TOKEN\",\n",
    "    )\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\n",
    "        \"granite_finetune\", tokenizer, quantization_method=\"q4_k_m\"\n",
    "    )\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\n",
    "        \"HF_USERNAME/granite_finetune\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\",\n",
    "        token=\"YOUR_HF_TOKEN\",\n",
    "    )\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"HF_USERNAME/granite_finetune\",  # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method=[\n",
    "            \"q4_k_m\",\n",
    "            \"q8_0\",\n",
    "            \"q5_k_m\",\n",
    "        ],\n",
    "        token=\"YOUR_HF_TOKEN\",  # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
